{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"datasetVersion","sourceId":10285892,"datasetId":6278249,"databundleVersionId":10587353}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# INTRODUCTION\n\nCompetition: https://codalab.lisn.upsaclay.fr/competitions/21112\n\nThis is a challenge associated to the PRIN 2022 project \"LICAM - AI-powered LiDAR fusion for next-generation smartphone cameras\". The challenge task is to deblur real low-light images taken by the iPhone 15 Pro, using both the blurred image and the co-registered depth map produced by the Lidar sensor. The deblurred images will be compared to registered ground truth sharp images by means of the LPIPS perceptual quality metric.\n\nTraining and validation data are provided from a novel dataset of low-light iPhone images affected by noise and motion blur, with a registered Lidar map and a sharp ground truth image. These images are the most similar to the test images. Participants may also use the ARKitScenes dataset to pretrain their models by simulating motion blur.\n\nThe LICAM -“AI-powered LIDAR fusion for next-generation smartphone cameras (LICAM)” project is funded by European Union – Next Generation EU within the PRIN 2022 program (D.D. 104 - 02/02/2022 Ministero dell’Università e della Ricerca). The contents of this website reflect only the authors' views and opinions and the Ministry cannot be considered responsible for them.","metadata":{"execution":{"iopub.status.busy":"2024-12-18T07:42:39.525677Z","iopub.execute_input":"2024-12-18T07:42:39.526091Z","iopub.status.idle":"2024-12-18T07:42:51.895756Z","shell.execute_reply.started":"2024-12-18T07:42:39.526056Z","shell.execute_reply":"2024-12-18T07:42:51.894183Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}}},{"cell_type":"code","source":"!pip install keras-unet-collection","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import glob\nfrom PIL import Image\nimport numpy as np\nimport re\nimport tensorflow as tf\nimport torch\nimport random\nfrom sklearn.model_selection import train_test_split\nfrom keras_unet_collection import models\n\nimport matplotlib.pyplot as plt\n\nfrom typing import Optional","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T07:43:08.264139Z","iopub.execute_input":"2024-12-18T07:43:08.265334Z","iopub.status.idle":"2024-12-18T07:43:08.309393Z","shell.execute_reply.started":"2024-12-18T07:43:08.265270Z","shell.execute_reply":"2024-12-18T07:43:08.308296Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"PATH_TRAIN: str = \"/kaggle/input/lidar-challenge/LICAM_deblur_challenge_dataset/train_val\"\nPATH_TEST: str = \"/kaggle/input/lidar-challenge/LICAM_deblur_challenge_dataset/test\"\nPATH_DATA: str = \"/kaggle/input/lidar-challenge/data\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Depth map preprocessing","metadata":{}},{"cell_type":"code","source":"MASK_VALUE: int = -1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"files_train = glob.glob(f\"{PATH_TRAIN}/images45/*/depth/*.png\")\nfiles_data = glob.glob(f\"{PATH_DATA}/*/*/highres_depth/*.png\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for file in files_train:\n    img = np.array(Image.open(file), dtype=np.int32)\n    img = (img - np.min(img)) / (np.max(img) - np.min(img))\n    np.save(f'./0_depth_{re.findall(r\".*/images45/.*/depth/(.*).png\", file)[0]}.npy', img)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for file in files_data:\n    img = np.array(Image.open(file).resize(size=(512, 512), resample=Image.NEAREST), dtype=np.int32)\n    img[img == 0] = MASK_VALUE\n    img[img != MASK_VALUE] = ((img[img != MASK_VALUE] - np.min(img[img != MASK_VALUE])) \n                              / (np.max(img[img != MASK_VALUE]) - np.min(img[img != MASK_VALUE])))\n    np.save(f'./1_depth_{re.findall(r\".*/.*/.*/highres_depth/(.*).png\", file)[0]}.npy', img)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Image preprocessing","metadata":{}},{"cell_type":"code","source":"files_train = glob.glob(f\"{PATH_TRAIN}/images45/*/rgb/*.png\")\nfiles_data = glob.glob(f\"{PATH_DATA}/*/*/wide/*.png\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for file in files_train:\n    img = np.array(Image.open(file), dtype=np.float32)\n    img = img / 255.\n    np.save(f'./0_rgb_{re.findall(r\".*/images45/.*/rgb/(.*).png\", file)[0]}.npy', img)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for file in files_data:\n    img = np.array(Image.open(file).resize(size=(512, 512), resample=Image.LANCZOS), dtype=np.int32)\n    img = img / 255.\n    np.save(f'./1_rgb_{re.findall(r\".*/.*/.*/wide/(.*).png\", file)[0]}.npy', img)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Ground truth","metadata":{}},{"cell_type":"code","source":"files_train = glob.glob(f\"{PATH_TRAIN}/images45/*/gt/*.png\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for file in files_train:\n    img = np.array(Image.open(file), dtype=np.float32)\n    img = img / 255.\n    np.save(f'./0_gt_{re.findall(r\".*/images45/.*/gt/(.*).png\", file)[0]}.npy', img)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Sequence generator","metadata":{}},{"cell_type":"code","source":"idx = glob.glob(f\"./*_rgb_*.npy\")\nidx = np.asarray([re.findall(r\"./(.*)_rgb_(.*).npy\", file)[0] for file in idx])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T07:42:21.135167Z","iopub.execute_input":"2024-12-18T07:42:21.137056Z","iopub.status.idle":"2024-12-18T07:42:21.150919Z","shell.execute_reply.started":"2024-12-18T07:42:21.136995Z","shell.execute_reply":"2024-12-18T07:42:21.149748Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def random_motion(steps: Optional[int] = 16,\n                  initial_vector: Optional[torch.Tensor] = None,\n                  alpha: Optional[float] = 0.2):\n    motion = [torch.zeros_like(initial_vector)]\n    for s in range(steps):\n        change = torch.randn(initial_vector.shape[0], dtype=torch.cfloat)\n        initial_vector = initial_vector + change * alpha\n        initial_vector /= initial_vector.abs().add(1e-8)\n        motion.append(motion[-1] + initial_vector)\n    motion = torch.stack(motion, -1)\n    motion -= motion.mean(-1, keepdim=True)\n    xrange = max(motion.real.max().ceil().long(), -motion.real.min().floor().long()) * 2\n    yrange = max(motion.imag.max().ceil().long(), -motion.imag.min().floor().long()) * 2\n    \n    kernel = torch.zeros(initial_vector.shape[0], 1, yrange.item()+1, xrange.item()+1)\n    for s in range(steps):\n        v = motion[:,s] + kernel.shape[-1] // 2 + (kernel.shape[-2] // 2)*1j\n        ixs = v.real.long() \n        iys = v.imag.long()\n        vxs = v.real - ixs\n        vys = v.imag - iys\n\n        for i, (iy, ix, vy, vx) in enumerate(zip(iys, ixs, vys, vxs)): \n            kernel[i,0,iy,ix] += (1-vx) * (1-vy) / steps\n            kernel[i,0,iy,ix+1] += vx * (1-vy) / steps\n            kernel[i,0,iy+1,ix] += (1-vx) * vy / steps\n            kernel[i,0,iy+1,ix+1] += vx * vy / steps\n        \n    return kernel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T07:42:23.241143Z","iopub.execute_input":"2024-12-18T07:42:23.241625Z","iopub.status.idle":"2024-12-18T07:42:23.253316Z","shell.execute_reply.started":"2024-12-18T07:42:23.241585Z","shell.execute_reply":"2024-12-18T07:42:23.252017Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class RandomMotionBlur(torch.nn.Module):\n    def __init__(self,\n                 steps: Optional[int] = 17,\n                 alpha: Optional[float] = 0.2):\n        super().__init__()\n        self.steps = steps\n        self.alpha = alpha\n        \n    def forward(self,\n                x: torch.Tensor):\n        x = x.swapaxes(1, 3)\n        vector = torch.randn(x.shape[0], dtype=torch.cfloat) / 3\n        vector.real /= 2\n        m = random_motion(self.steps, vector, alpha=self.alpha)\n        xpad = [m.shape[-1]//2+1] * 2 + [m.shape[-2]//2+1] * 2\n        x = torch.nn.functional.pad(x, xpad)\n        mpad = [0, x.shape[-1]-m.shape[-1], 0, x.shape[-2]-m.shape[-2]]\n        mp = torch.nn.functional.pad(m, mpad)\n        fx = torch.fft.fft2(x)\n        fm = torch.fft.fft2(mp)\n        fy = fx * fm\n        y = torch.fft.ifft2(fy).real\n        y = y[...,xpad[2]:-xpad[3], xpad[0]:-xpad[1]].swapaxes(1, 3)\n        return y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T07:42:24.209154Z","iopub.execute_input":"2024-12-18T07:42:24.209573Z","iopub.status.idle":"2024-12-18T07:42:24.219568Z","shell.execute_reply.started":"2024-12-18T07:42:24.209515Z","shell.execute_reply":"2024-12-18T07:42:24.218310Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DataGenerator(tf.keras.utils.Sequence):\n    def __init__(self, \n                 idx: np.ndarray,\n                 batch_size: Optional[int] = 8,\n                 shuffle: Optional[bool] = True,\n                 **kwargs):\n        super(tf.keras.utils.Sequence, self).__init__(**kwargs)\n        self.idx = idx\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n\n    def __len__(self):\n        return int(np.ceil(len(self.idx) / self.batch_size))\n\n    def on_epoch_end(self):\n        if self.shuffle:\n            np.random.shuffle(self.idx)\n\n    def __getitem__(self,\n                    index: int):\n        idx = self.idx[index * self.batch_size : (index + 1) * self.batch_size]\n        X_rgb = np.zeros((len(idx), 512, 512, 3), dtype=np.float32)\n        X_depth = np.zeros((len(idx), 512, 512), dtype=np.float32)\n        y = np.zeros((len(idx), 512, 512, 3), dtype=np.float32)\n        for i, (dtype, file) in enumerate(idx):\n            X_depth[i] = np.load(f\"./{dtype}_depth_{file}.npy\")\n            X_rgb[i] = np.load(f\"./{dtype}_rgb_{file}.npy\")\n            y[i] = np.load(f\"./{dtype}_gt_{file}.npy\") if dtype == \"0\" else np.load(f\"./{dtype}_rgb_{file}.npy\")\n            if dtype == \"1\":\n                blur = RandomMotionBlur(steps=random.randint(5, 50))\n                X_rgb[i] = blur(torch.Tensor(X_rgb[i][np.newaxis, ...]))[0]\n        return (X_rgb, X_depth[..., np.newaxis]), y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T07:42:25.065545Z","iopub.execute_input":"2024-12-18T07:42:25.067158Z","iopub.status.idle":"2024-12-18T07:42:25.176158Z","shell.execute_reply.started":"2024-12-18T07:42:25.067090Z","shell.execute_reply":"2024-12-18T07:42:25.174872Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"idx_train, idx_test = train_test_split(range(len(idx)), test_size=0.3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T07:42:26.716300Z","iopub.execute_input":"2024-12-18T07:42:26.717034Z","iopub.status.idle":"2024-12-18T07:42:26.724882Z","shell.execute_reply.started":"2024-12-18T07:42:26.716997Z","shell.execute_reply":"2024-12-18T07:42:26.723584Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_generator = DataGenerator(idx=idx[idx_train])\ntest_generator = DataGenerator(idx=idx[idx_test])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T07:42:27.961359Z","iopub.execute_input":"2024-12-18T07:42:27.961873Z","iopub.status.idle":"2024-12-18T07:42:27.967759Z","shell.execute_reply.started":"2024-12-18T07:42:27.961837Z","shell.execute_reply":"2024-12-18T07:42:27.966487Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"unet = models.unet_2d((512, 512, 4),\n                      [16, 32, 64],\n                      n_labels=3,\n                      stack_num_down=1,\n                      stack_num_up=1,\n                      activation='Snake',\n                      output_activation=None, \n                      batch_norm=True,\n                      pool='max',\n                      unpool='nearest',\n                      name='unet')\nunet.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T07:46:36.907999Z","iopub.execute_input":"2024-12-18T07:46:36.908993Z","iopub.status.idle":"2024-12-18T07:46:37.080985Z","shell.execute_reply.started":"2024-12-18T07:46:36.908948Z","shell.execute_reply":"2024-12-18T07:46:37.079981Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_rgb = tf.keras.Input(shape=(512, 512, 3), dtype=tf.float32)\ninput_depth = tf.keras.Input(shape=(512, 512, 1), dtype=tf.float32)\nx = tf.keras.layers.concatenate([input_rgb, input_depth], axis=3)\noutputs = unet(x)\nmodel = tf.keras.models.Model(inputs=[input_rgb, input_depth],\n                              outputs=outputs)\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T07:46:57.724916Z","iopub.execute_input":"2024-12-18T07:46:57.725286Z","iopub.status.idle":"2024-12-18T07:46:57.757090Z","shell.execute_reply.started":"2024-12-18T07:46:57.725257Z","shell.execute_reply":"2024-12-18T07:46:57.756239Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class VGGFeatureMatchingLoss(tf.keras.losses.Loss):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.encoder_layers = [\"block1_conv1\",\n                               \"block2_conv1\",\n                               \"block3_conv1\",\n                               \"block4_conv1\",\n                               \"block5_conv1\"]\n        self.weights = [1.0 / 32, 1.0 / 16, 1.0 / 8, 1.0 / 4, 1.0]\n        vgg = tf.keras.applications.VGG19(include_top=False, weights=\"imagenet\")\n        layer_outputs = [vgg.get_layer(x).output for x in self.encoder_layers]\n        self.vgg_model = tf.keras.Model(vgg.input, layer_outputs, name=\"VGG\")\n        self.mae = tf.keras.losses.MeanAbsoluteError()\n\n    def call(self, y_true, y_pred):\n        y_true = tf.keras.applications.vgg19.preprocess_input(255. * y_true)\n        y_pred = tf.keras.applications.vgg19.preprocess_input(255. * y_pred)\n        real_features = self.vgg_model(y_true)\n        fake_features = self.vgg_model(y_pred)\n        loss = 0\n        for i in range(len(real_features)):\n            loss += self.weights[i] * self.mae(real_features[i], fake_features[i])\n        return loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T07:47:04.725910Z","iopub.execute_input":"2024-12-18T07:47:04.726293Z","iopub.status.idle":"2024-12-18T07:47:04.735113Z","shell.execute_reply.started":"2024-12-18T07:47:04.726262Z","shell.execute_reply":"2024-12-18T07:47:04.734008Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class PerceptualLoss(tf.keras.losses.Loss):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.mae = tf.keras.losses.MeanAbsoluteError()\n        self.mse = tf.keras.losses.MeanSquaredError()\n        self.vggloss = VGGFeatureMatchingLoss()\n\n    def call(self, y_true, y_pred):\n        loss = (self.vggloss(y_true, y_pred) * 1.5\n                + self.mae(y_true, y_pred) * 0.5\n                + self.mse(y_true, y_pred) * 0.5\n                + (1 - tf.reduce_mean(tf.image.ssim(tf.clip_by_value(y_true, clip_value_max=1.0, clip_value_min=0.0), \n                                                    tf.clip_by_value(y_pred, clip_value_max=1.0, clip_value_min=0.0), \n                                                    1.0))) * 0.7)\n        return loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T07:47:06.542437Z","iopub.execute_input":"2024-12-18T07:47:06.542889Z","iopub.status.idle":"2024-12-18T07:47:06.550953Z","shell.execute_reply.started":"2024-12-18T07:47:06.542851Z","shell.execute_reply":"2024-12-18T07:47:06.549804Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n              loss=PerceptualLoss(),\n              metrics=[\"mae\", \"mse\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T07:47:07.980674Z","iopub.execute_input":"2024-12-18T07:47:07.981117Z","iopub.status.idle":"2024-12-18T07:47:09.291027Z","shell.execute_reply.started":"2024-12-18T07:47:07.981079Z","shell.execute_reply":"2024-12-18T07:47:09.290037Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.fit(train_generator,\n          validation_data=test_generator,\n          epochs=10,\n          verbose=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T07:47:10.853885Z","iopub.execute_input":"2024-12-18T07:47:10.854288Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Predict","metadata":{}},{"cell_type":"code","source":"X_rgb = np.asarray(Image.open(f\"{PATH_TEST}/1/rgb/1.png\"))[np.newaxis, ...]\nX_depth = np.asarray(Image.open(f\"{PATH_TEST}/1/depth/1.png\"))[np.newaxis, ...]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = model.predict([X_rgb, X_depth])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = np.clip(y_pred, a_max=1., a_min=0.) * 255.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.imshow(y_pred[0])\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}