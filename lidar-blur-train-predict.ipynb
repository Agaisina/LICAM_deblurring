{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10285892,"sourceType":"datasetVersion","datasetId":6278249},{"sourceId":10396915,"sourceType":"datasetVersion","datasetId":6441956},{"sourceId":248823,"sourceType":"modelInstanceVersion","modelInstanceId":187375,"modelId":209450}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**INTRODUCTION**\n\nCompetition: https://codalab.lisn.upsaclay.fr/competitions/21112\n\nThis is a challenge associated to the PRIN 2022 project \"LICAM - AI-powered LiDAR fusion for next-generation smartphone cameras\". The challenge task is to deblur real low-light images taken by the iPhone 15 Pro, using both the blurred image and the co-registered depth map produced by the Lidar sensor. The deblurred images will be compared to registered ground truth sharp images by means of the LPIPS perceptual quality metric.\n\nTraining and validation data are provided from a novel dataset of low-light iPhone images affected by noise and motion blur, with a registered Lidar map and a sharp ground truth image. These images are the most similar to the test images. Participants may also use the ARKitScenes dataset to pretrain their models by simulating motion blur.\n\nThe LICAM -“AI-powered LIDAR fusion for next-generation smartphone cameras (LICAM)” project is funded by European Union – Next Generation EU within the PRIN 2022 program (D.D. 104 - 02/02/2022 Ministero dell’Università e della Ricerca). The contents of this website reflect only the authors' views and opinions and the Ministry cannot be considered responsible for them.","metadata":{"execution":{"iopub.status.busy":"2025-02-02T14:09:48.158955Z","iopub.execute_input":"2025-02-02T14:09:48.159280Z","iopub.status.idle":"2025-02-02T14:10:32.505987Z","shell.execute_reply.started":"2025-02-02T14:09:48.159254Z","shell.execute_reply":"2025-02-02T14:10:32.504960Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}}},{"cell_type":"code","source":"!pip install keras-unet-collection\n# !pip install tensorflow==2.13.0\n# !pip install tensorflow-gpu==2.8.0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import glob\nfrom PIL import Image\nimport numpy as np\nimport re\nimport tensorflow as tf\nimport transformers\nimport torch\nimport random\nfrom sklearn.model_selection import train_test_split\nfrom keras_unet_collection import models\n\nimport matplotlib.pyplot as plt\n\nfrom typing import (Optional,\n                    Tuple)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:36:21.765761Z","iopub.execute_input":"2025-02-03T09:36:21.766148Z","iopub.status.idle":"2025-02-03T09:36:37.587666Z","shell.execute_reply.started":"2025-02-03T09:36:21.766119Z","shell.execute_reply":"2025-02-03T09:36:37.586784Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"PATH_TRAIN: str = \"../input/lidar-challenge/LICAM_deblur_challenge_dataset/train_val\"\nPATH_TEST: str = \"../input/lidar-challenge/LICAM_deblur_challenge_dataset/test\"\nPATH_DATA: str = \"../input/lidar-challenge/data\"\nPATH_MODEL: str = \"../input/deblurring_images/other/default/4/best_model_unet.weights.h5\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:42:51.340562Z","iopub.execute_input":"2025-02-03T09:42:51.341233Z","iopub.status.idle":"2025-02-03T09:42:51.345312Z","shell.execute_reply.started":"2025-02-03T09:42:51.341202Z","shell.execute_reply":"2025-02-03T09:42:51.344494Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Depth map preprocessing","metadata":{}},{"cell_type":"code","source":"MASK_VALUE: int = -1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:42:51.653341Z","iopub.execute_input":"2025-02-03T09:42:51.653664Z","iopub.status.idle":"2025-02-03T09:42:51.657359Z","shell.execute_reply.started":"2025-02-03T09:42:51.653640Z","shell.execute_reply":"2025-02-03T09:42:51.656487Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"files_train = glob.glob(f\"{PATH_TRAIN}/images45/*/depth/*.png\")\nfiles_data = glob.glob(f\"{PATH_DATA}/*/*/highres_depth/*.png\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:42:51.797288Z","iopub.execute_input":"2025-02-03T09:42:51.797587Z","iopub.status.idle":"2025-02-03T09:42:53.182936Z","shell.execute_reply.started":"2025-02-03T09:42:51.797562Z","shell.execute_reply":"2025-02-03T09:42:53.182062Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for file in files_train:\n    img = np.array(Image.open(file), dtype=np.int32)\n    img = (img - np.min(img)) / (np.max(img) - np.min(img))\n    np.save(f'./0_depth_{re.findall(r\".*/images45/.*/depth/(.*).png\", file)[0]}.npy', img)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:42:53.184062Z","iopub.execute_input":"2025-02-03T09:42:53.184314Z","iopub.status.idle":"2025-02-03T09:42:53.669435Z","shell.execute_reply.started":"2025-02-03T09:42:53.184295Z","shell.execute_reply":"2025-02-03T09:42:53.668795Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#for file in files_data:\n#    img = np.array(Image.open(file).resize(size=(512, 512), resample=Image.NEAREST), dtype=np.int32)\n#    img[img == 0] = MASK_VALUE\n#    img[img != MASK_VALUE] = ((img[img != MASK_VALUE] - np.min(img[img != MASK_VALUE])) \n#                              / (np.max(img[img != MASK_VALUE]) - np.min(img[img != MASK_VALUE])))\n#    np.save(f'./1_depth_{re.findall(r\".*/.*/.*/highres_depth/(.*).png\", file)[0]}.npy', img)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:42:53.670602Z","iopub.execute_input":"2025-02-03T09:42:53.670870Z","iopub.status.idle":"2025-02-03T09:42:53.673980Z","shell.execute_reply.started":"2025-02-03T09:42:53.670822Z","shell.execute_reply":"2025-02-03T09:42:53.673314Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for file in files_data:\n    img = np.array(Image.open(file).resize(size=(512, 512), resample=Image.NEAREST), dtype=np.int32)\n    img[img == 0] = np.mean(img[img != 0])\n    img = (img - np.min(img)) / (np.max(img) - np.min(img))\n    np.save(f'./1_depth_{re.findall(r\".*/.*/.*/highres_depth/(.*).png\", file)[0]}.npy', img)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:42:54.812347Z","iopub.execute_input":"2025-02-03T09:42:54.812625Z","iopub.status.idle":"2025-02-03T09:43:31.310258Z","shell.execute_reply.started":"2025-02-03T09:42:54.812604Z","shell.execute_reply":"2025-02-03T09:43:31.309554Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Image preprocessing","metadata":{}},{"cell_type":"code","source":"files_train = glob.glob(f\"{PATH_TRAIN}/images45/*/rgb/*.png\")\nfiles_data = glob.glob(f\"{PATH_DATA}/*/*/wide/*.png\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:43:31.311300Z","iopub.execute_input":"2025-02-03T09:43:31.311588Z","iopub.status.idle":"2025-02-03T09:43:32.144375Z","shell.execute_reply.started":"2025-02-03T09:43:31.311565Z","shell.execute_reply":"2025-02-03T09:43:32.143700Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for file in files_train:\n    img = np.array(Image.open(file), dtype=np.float32)\n    img = img / 255.\n    np.save(f'./0_rgb_{re.findall(r\".*/images45/.*/rgb/(.*).png\", file)[0]}.npy', img)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:43:32.145591Z","iopub.execute_input":"2025-02-03T09:43:32.145889Z","iopub.status.idle":"2025-02-03T09:43:33.009332Z","shell.execute_reply.started":"2025-02-03T09:43:32.145862Z","shell.execute_reply":"2025-02-03T09:43:33.008658Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for file in files_data:\n    img = np.array(Image.open(file).resize(size=(512, 512), resample=Image.LANCZOS), dtype=np.int32)\n    img = img / 255.\n    np.save(f'./1_rgb_{re.findall(r\".*/.*/.*/wide/(.*).png\", file)[0]}.npy', img)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:43:33.010602Z","iopub.execute_input":"2025-02-03T09:43:33.010868Z","iopub.status.idle":"2025-02-03T09:45:11.144845Z","shell.execute_reply.started":"2025-02-03T09:43:33.010814Z","shell.execute_reply":"2025-02-03T09:45:11.143928Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Ground truth","metadata":{}},{"cell_type":"code","source":"files_train = glob.glob(f\"{PATH_TRAIN}/images45/*/gt/*.png\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:45:11.145753Z","iopub.execute_input":"2025-02-03T09:45:11.146078Z","iopub.status.idle":"2025-02-03T09:45:11.656784Z","shell.execute_reply.started":"2025-02-03T09:45:11.146048Z","shell.execute_reply":"2025-02-03T09:45:11.655920Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for file in files_train:\n    img = np.array(Image.open(file), dtype=np.float32)\n    img = img / 255.\n    np.save(f'./0_gt_{re.findall(r\".*/images45/.*/gt/(.*).png\", file)[0]}.npy', img)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:45:11.657745Z","iopub.execute_input":"2025-02-03T09:45:11.658070Z","iopub.status.idle":"2025-02-03T09:45:12.527552Z","shell.execute_reply.started":"2025-02-03T09:45:11.658043Z","shell.execute_reply":"2025-02-03T09:45:12.526915Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Sequence generator","metadata":{}},{"cell_type":"code","source":"idx = glob.glob(f\"./*_rgb_*.npy\")\nidx = np.asarray([re.findall(r\"./(.*)_rgb_(.*).npy\", file)[0] for file in idx])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:45:12.528242Z","iopub.execute_input":"2025-02-03T09:45:12.528460Z","iopub.status.idle":"2025-02-03T09:45:12.537949Z","shell.execute_reply.started":"2025-02-03T09:45:12.528441Z","shell.execute_reply":"2025-02-03T09:45:12.537140Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def random_motion(steps: Optional[int] = 16,\n                  initial_vector: Optional[torch.Tensor] = None,\n                  alpha: Optional[float] = 0.2):\n    motion = [torch.zeros_like(initial_vector)]\n    for s in range(steps):\n        change = torch.randn(initial_vector.shape[0], dtype=torch.cfloat)\n        initial_vector = initial_vector + change * alpha\n        initial_vector /= initial_vector.abs().add(1e-8)\n        motion.append(motion[-1] + initial_vector)\n    motion = torch.stack(motion, -1)\n    motion -= motion.mean(-1, keepdim=True)\n    xrange = max(motion.real.max().ceil().long(), -motion.real.min().floor().long()) * 2\n    yrange = max(motion.imag.max().ceil().long(), -motion.imag.min().floor().long()) * 2\n    \n    kernel = torch.zeros(initial_vector.shape[0], 1, yrange.item()+1, xrange.item()+1)\n    for s in range(steps):\n        v = motion[:,s] + kernel.shape[-1] // 2 + (kernel.shape[-2] // 2)*1j\n        ixs = v.real.long() \n        iys = v.imag.long()\n        vxs = v.real - ixs\n        vys = v.imag - iys\n\n        for i, (iy, ix, vy, vx) in enumerate(zip(iys, ixs, vys, vxs)): \n            kernel[i,0,iy,ix] += (1-vx) * (1-vy) / steps\n            kernel[i,0,iy,ix+1] += vx * (1-vy) / steps\n            kernel[i,0,iy+1,ix] += (1-vx) * vy / steps\n            kernel[i,0,iy+1,ix+1] += vx * vy / steps\n        \n    return kernel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:45:12.539909Z","iopub.execute_input":"2025-02-03T09:45:12.540112Z","iopub.status.idle":"2025-02-03T09:45:12.548234Z","shell.execute_reply.started":"2025-02-03T09:45:12.540094Z","shell.execute_reply":"2025-02-03T09:45:12.547490Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class RandomMotionBlur(torch.nn.Module):\n    def __init__(self,\n                 steps: Optional[int] = 17,\n                 alpha: Optional[float] = 0.2):\n        super().__init__()\n        self.steps = steps\n        self.alpha = alpha\n        \n    def forward(self,\n                x: torch.Tensor):\n        x = x.swapaxes(1, 3)\n        vector = torch.randn(x.shape[0], dtype=torch.cfloat) / 3\n        vector.real /= 2\n        m = random_motion(self.steps, vector, alpha=self.alpha)\n        xpad = [m.shape[-1]//2+1] * 2 + [m.shape[-2]//2+1] * 2\n        x = torch.nn.functional.pad(x, xpad)\n        mpad = [0, x.shape[-1]-m.shape[-1], 0, x.shape[-2]-m.shape[-2]]\n        mp = torch.nn.functional.pad(m, mpad)\n        fx = torch.fft.fft2(x)\n        fm = torch.fft.fft2(mp)\n        fy = fx * fm\n        y = torch.fft.ifft2(fy).real\n        y = y[...,xpad[2]:-xpad[3], xpad[0]:-xpad[1]].swapaxes(1, 3)\n        return y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:45:15.010812Z","iopub.execute_input":"2025-02-03T09:45:15.011171Z","iopub.status.idle":"2025-02-03T09:45:15.017571Z","shell.execute_reply.started":"2025-02-03T09:45:15.011146Z","shell.execute_reply":"2025-02-03T09:45:15.016708Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DataGenerator(tf.keras.utils.Sequence):\n    def __init__(self, \n                 idx: np.ndarray,\n                 batch_size: Optional[int] = 8,\n                 shuffle: Optional[bool] = True,\n                 **kwargs):\n        super(DataGenerator, self).__init__(**kwargs)\n        self.idx = idx\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n\n    def __len__(self):\n        return int(np.ceil(len(self.idx) / self.batch_size))\n\n    def on_epoch_end(self):\n        if self.shuffle:\n            np.random.shuffle(self.idx)\n\n    def __getitem__(self,\n                    index: int):\n        idx = self.idx[index * self.batch_size : (index + 1) * self.batch_size]\n        X_rgb = np.zeros((len(idx), 512, 512, 3), dtype=np.float32)\n        X_depth = np.zeros((len(idx), 512, 512), dtype=np.float32)\n        y = np.zeros((len(idx), 512, 512, 3), dtype=np.float32)\n        for i, (dtype, file) in enumerate(idx):\n            X_depth[i] = np.load(f\"./{dtype}_depth_{file}.npy\")\n            X_rgb[i] = np.load(f\"./{dtype}_rgb_{file}.npy\")\n            y[i] = np.load(f\"./{dtype}_gt_{file}.npy\") if dtype == \"0\" else np.load(f\"./{dtype}_rgb_{file}.npy\")\n            if dtype == \"1\":\n                blur = RandomMotionBlur(steps=random.randint(5, 50))\n                X_rgb[i] = blur(torch.Tensor(X_rgb[i][np.newaxis, ...]))[0]\n        return (X_rgb, X_depth[..., np.newaxis]), y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:45:15.173019Z","iopub.execute_input":"2025-02-03T09:45:15.173330Z","iopub.status.idle":"2025-02-03T09:45:15.181741Z","shell.execute_reply.started":"2025-02-03T09:45:15.173307Z","shell.execute_reply":"2025-02-03T09:45:15.180950Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"idx_train, idx_test = train_test_split(range(len(idx)), test_size=0.3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:45:15.304624Z","iopub.execute_input":"2025-02-03T09:45:15.305080Z","iopub.status.idle":"2025-02-03T09:45:15.309763Z","shell.execute_reply.started":"2025-02-03T09:45:15.305043Z","shell.execute_reply":"2025-02-03T09:45:15.308943Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_generator = DataGenerator(idx=idx[idx_train], batch_size=4)\ntest_generator = DataGenerator(idx=idx[idx_test], batch_size=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:45:15.455244Z","iopub.execute_input":"2025-02-03T09:45:15.455531Z","iopub.status.idle":"2025-02-03T09:45:15.459478Z","shell.execute_reply.started":"2025-02-03T09:45:15.455507Z","shell.execute_reply":"2025-02-03T09:45:15.458740Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class SPADE(tf.keras.layers.Layer):\n    def __init__(self, \n                 filters: Tuple[int],\n                 epsilon: Optional[float] = 1e-5,\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.epsilon = epsilon\n        self.conv = tf.keras.layers.Conv2D(128, 3, padding=\"same\", activation=\"gelu\")\n        self.conv_gamma = tf.keras.layers.Conv2D(filters, 3, padding=\"same\")\n        self.conv_beta = tf.keras.layers.Conv2D(filters, 3, padding=\"same\")\n\n    def build(self,\n              input_shape: Tuple[int]):\n        self.resize_shape = input_shape[1:3]\n\n    def call(self,\n             input_tensor: tf.Tensor,\n             raw_mask: tf.Tensor):\n        mask = tf.keras.ops.image.resize(raw_mask, self.resize_shape, interpolation=\"nearest\")\n        x = self.conv(mask)\n        gamma = self.conv_gamma(x)\n        beta = self.conv_beta(x)\n        mean, var = tf.keras.ops.moments(input_tensor, axes=(0, 1, 2), keepdims=True)\n        std = tf.keras.ops.sqrt(var + self.epsilon)\n        normalized = (input_tensor - mean) / std\n        output = gamma * normalized + beta\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:45:20.956949Z","iopub.execute_input":"2025-02-03T09:45:20.957275Z","iopub.status.idle":"2025-02-03T09:45:20.963602Z","shell.execute_reply.started":"2025-02-03T09:45:20.957248Z","shell.execute_reply":"2025-02-03T09:45:20.962700Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ResBlock(tf.keras.layers.Layer):\n    def __init__(self,\n                 filters: Tuple[int],\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.filters = filters\n\n    def build(self,\n              input_shape: Tuple[int]):\n        input_filter = input_shape[-1]\n        self.spade_1 = SPADE(input_filter)\n        self.spade_2 = SPADE(self.filters)\n        self.conv_1 = tf.keras.layers.Conv2D(self.filters, 3, padding=\"same\")\n        self.conv_2 = tf.keras.layers.Conv2D(self.filters, 3, padding=\"same\")\n        self.learned_skip = False\n\n        if self.filters != input_filter:\n            self.learned_skip = True\n            self.spade_3 = SPADE(input_filter)\n            self.conv_3 = tf.keras.layers.Conv2D(self.filters, 3, padding=\"same\")\n\n    def call(self, input_tensor, mask):\n        x = self.spade_1(input_tensor, mask)\n        x = self.conv_1(tf.keras.activations.leaky_relu(x, 0.2))\n        x = self.spade_2(x, mask)\n        x = self.conv_2(tf.keras.activations.leaky_relu(x, 0.2))\n        skip = (self.conv_3(tf.keras.activations.leaky_relu(self.spade_3(input_tensor, mask), 0.2))\n                if self.learned_skip\n                else input_tensor)\n        output = skip + x\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:45:21.097651Z","iopub.execute_input":"2025-02-03T09:45:21.098015Z","iopub.status.idle":"2025-02-03T09:45:21.105227Z","shell.execute_reply.started":"2025-02-03T09:45:21.097983Z","shell.execute_reply":"2025-02-03T09:45:21.103991Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SPADEBlock(tf.keras.layers.Layer):\n    def __init__(self,\n                 filters: Tuple[int],\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.filters = filters\n\n    def build(self,\n              input_shape: Tuple[int]):\n        input_filter = input_shape[-1]\n        self.spade_1 = SPADE(input_filter)\n        self.spade_2 = SPADE(self.filters)\n        self.conv_1 = tf.keras.layers.Conv2D(self.filters, 3, padding=\"same\")\n        self.conv_2 = tf.keras.layers.Conv2D(self.filters, 3, padding=\"same\")\n\n    def call(self, input_tensor, mask):\n        x = self.spade_1(input_tensor, mask)\n        x = self.conv_1(tf.keras.activations.leaky_relu(x, 0.2))\n        x = self.spade_2(x, mask)\n        x = self.conv_2(tf.keras.activations.leaky_relu(x, 0.2))\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:45:21.229042Z","iopub.execute_input":"2025-02-03T09:45:21.229369Z","iopub.status.idle":"2025-02-03T09:45:21.234937Z","shell.execute_reply.started":"2025-02-03T09:45:21.229341Z","shell.execute_reply":"2025-02-03T09:45:21.234091Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def downsample_spadeblock(x: tf.Tensor,\n                        mask: tf.Tensor,\n                        channels: int,\n                        kernels: Tuple[int],\n                        strides: Optional[int] = 2):\n    x = SPADEBlock(filters=channels) (x, mask)\n    x = tf.keras.layers.Conv2D(channels,\n                               kernels,\n                               strides=strides,\n                               padding=\"same\",\n                               use_bias=False,\n                               kernel_initializer=tf.keras.initializers.GlorotNormal()) (x)\n    x = tf.keras.layers.GroupNormalization(groups=-1) (x)\n    x = tf.keras.layers.LeakyReLU(0.2) (x)\n    return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:45:21.369683Z","iopub.execute_input":"2025-02-03T09:45:21.370000Z","iopub.status.idle":"2025-02-03T09:45:21.374777Z","shell.execute_reply.started":"2025-02-03T09:45:21.369975Z","shell.execute_reply":"2025-02-03T09:45:21.373952Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def downsample(channels: int,\n               kernels: Tuple[int],\n               strides: Optional[int] = 2):\n    block = tf.keras.Sequential()\n    block.add(tf.keras.layers.Conv2D(channels,\n                                     kernels,\n                                     strides=strides,\n                                     padding=\"same\",\n                                     use_bias=False,\n                                     kernel_initializer=tf.keras.initializers.GlorotNormal()))\n    block.add(tf.keras.layers.GroupNormalization(groups=-1))\n    block.add(tf.keras.layers.LeakyReLU(0.2))\n    return block","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:45:21.512492Z","iopub.execute_input":"2025-02-03T09:45:21.512778Z","iopub.status.idle":"2025-02-03T09:45:21.517564Z","shell.execute_reply.started":"2025-02-03T09:45:21.512757Z","shell.execute_reply":"2025-02-03T09:45:21.516795Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Basic","metadata":{}},{"cell_type":"code","source":"input_img = tf.keras.Input(shape=(512, 512, 3), dtype=tf.float32)\ninput_depth = tf.keras.Input(shape=(512, 512, 1), dtype=tf.float32)\nx = tf.keras.layers.concatenate([input_img, input_depth], axis=-1)\nx = tf.keras.layers.Conv2DTranspose(filters=16,\n                                    kernel_size=(7, 7),\n                                    strides=(1, 1),\n                                    padding=\"same\",\n                                    use_bias=False,\n                                    activation=None,\n                                    kernel_initializer=tf.keras.initializers.GlorotNormal()) (x)\nx = tf.keras.layers.GroupNormalization(groups=-1) (x)\nx = tf.keras.layers.LeakyReLU(0.2) (x)\nx = tf.keras.layers.Conv2DTranspose(filters=32,\n                                    kernel_size=(5, 5),\n                                    strides=(1, 1),\n                                    padding=\"same\",\n                                    use_bias=False,\n                                    activation=None,\n                                    kernel_initializer=tf.keras.initializers.GlorotNormal()) (x)\nx = tf.keras.layers.GroupNormalization(groups=-1) (x)\nx = tf.keras.layers.LeakyReLU(0.2) (x)\nx = tf.keras.layers.Conv2DTranspose(filters=64,\n                                    kernel_size=(3, 3),\n                                    strides=(1, 1),\n                                    padding=\"same\",\n                                    use_bias=False,\n                                    activation=None,\n                                    kernel_initializer=tf.keras.initializers.GlorotNormal()) (x)\nx = tf.keras.layers.GroupNormalization(groups=-1) (x)\nx = tf.keras.layers.LeakyReLU(0.2) (x)\nx = tf.keras.layers.Conv2DTranspose(filters=128,\n                                    kernel_size=(1, 1),\n                                    strides=(1, 1),\n                                    padding=\"same\",\n                                    use_bias=False,\n                                    activation=None,\n                                    kernel_initializer=tf.keras.initializers.GlorotNormal()) (x)\nx = tf.keras.layers.GroupNormalization(groups=-1) (x)\nx = tf.keras.layers.LeakyReLU(0.2) (x)\nx = tf.keras.layers.Conv2DTranspose(filters=3,\n                                    kernel_size=(1, 1),\n                                    strides=(1, 1),\n                                    padding=\"same\",\n                                    use_bias=False,\n                                    activation=\"tanh\",\n                                    kernel_initializer=tf.keras.initializers.GlorotNormal()) (x)\n\nmodel = tf.keras.models.Model(inputs=[input_img, input_depth],\n                              outputs=x)\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:45:25.966564Z","iopub.execute_input":"2025-02-03T09:45:25.966928Z","iopub.status.idle":"2025-02-03T09:45:28.069558Z","shell.execute_reply.started":"2025-02-03T09:45:25.966893Z","shell.execute_reply":"2025-02-03T09:45:28.068904Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Basic SPADE","metadata":{}},{"cell_type":"code","source":"input_img = tf.keras.Input(shape=(512, 512, 3), dtype=tf.float32)\ninput_depth = tf.keras.Input(shape=(512, 512, 1), dtype=tf.float32)\nx = SPADEBlock(filters=32) (input_img, input_depth)\nx = SPADEBlock(filters=64) (x, input_depth)\nx = SPADEBlock(filters=128) (x, input_depth)\nx = SPADEBlock(filters=256) (x, input_depth)\nx = tf.keras.layers.Conv2DTranspose(filters=3,\n                                    kernel_size=(1, 1),\n                                    strides=(1, 1),\n                                    padding=\"same\",\n                                    use_bias=False,\n                                    activation=\"tanh\",\n                                    kernel_initializer=tf.keras.initializers.GlorotNormal()) (x)\n\nmodel = tf.keras.models.Model(inputs=[input_img, input_depth],\n                              outputs=x)\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:45:28.070528Z","iopub.execute_input":"2025-02-03T09:45:28.070740Z","iopub.status.idle":"2025-02-03T09:45:28.628058Z","shell.execute_reply.started":"2025-02-03T09:45:28.070722Z","shell.execute_reply":"2025-02-03T09:45:28.627381Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## UNet -> esta","metadata":{}},{"cell_type":"code","source":"input_img = tf.keras.Input(shape=(512, 512, 3), dtype=tf.float32)\ninput_depth = tf.keras.Input(shape=(512, 512, 1), dtype=tf.float32)\nx = tf.keras.layers.concatenate([input_img, input_depth], axis=-1)\nx = downsample(channels=32,\n               kernels=(5, 5),\n               strides=(1, 1)) (x)\nd1 = downsample(channels=64,\n                kernels=(3, 3),\n                strides=(2, 2)) (x)\nd2 = downsample(channels=128,\n                kernels=(3, 3),\n                strides=(2, 2)) (d1)\nd3 = downsample(channels=256,\n                kernels=(3, 3),\n                strides=(2, 2)) (d2)\nd4 = downsample(channels=512,\n                kernels=(3, 3),\n                strides=(2, 2)) (d3)\nu4 = ResBlock(filters=512) (d4, input_depth)\nu4 = ResBlock(filters=512) (u4, input_depth)\nu4 = tf.keras.layers.UpSampling2D(size=(2, 2),\n                                  interpolation=\"nearest\") (u4)\nu3 = tf.keras.layers.concatenate([u4, d3])\nu3 = ResBlock(filters=256) (u3, input_depth)\nu3 = ResBlock(filters=256) (u3, input_depth)\nu3 = tf.keras.layers.UpSampling2D(size=(2, 2),\n                                  interpolation=\"nearest\") (u3)\nu2 = tf.keras.layers.concatenate([u3, d2])\nu2 = ResBlock(filters=128) (u2, input_depth)\nu2 = ResBlock(filters=128) (u2, input_depth)\nu2 = tf.keras.layers.UpSampling2D(size=(2, 2),\n                                  interpolation=\"nearest\") (u2)\nu1 = tf.keras.layers.concatenate([u2, d1])\nu1 = ResBlock(filters=64) (u1, input_depth)\nu1 = ResBlock(filters=64) (u1, input_depth)\nu1 = tf.keras.layers.UpSampling2D(size=(2, 2),\n                                  interpolation=\"nearest\") (u1)\nx = tf.keras.layers.concatenate([u1, x])\nx = ResBlock(filters=32) (x, input_depth)\nx = ResBlock(filters=32) (x, input_depth)\nx = tf.keras.layers.Conv2D(filters=3,\n                           kernel_size=(1, 1),\n                           strides=(1, 1),\n                           padding=\"same\",\n                           use_bias=False,\n                           activation=\"sigmoid\",\n                           kernel_initializer=tf.keras.initializers.GlorotNormal()) (x)\n\nmodel = tf.keras.models.Model(inputs=[input_img, input_depth],\n                              outputs=x)\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:45:32.350374Z","iopub.execute_input":"2025-02-03T09:45:32.350677Z","iopub.status.idle":"2025-02-03T09:45:33.982675Z","shell.execute_reply.started":"2025-02-03T09:45:32.350650Z","shell.execute_reply":"2025-02-03T09:45:33.982060Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"class VGGFeatureMatchingLoss(tf.keras.losses.Loss):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.encoder_layers = [\"block1_conv1\",\n                               \"block2_conv1\",\n                               \"block3_conv1\",\n                               \"block4_conv1\",\n                               \"block5_conv1\"]\n        self.weights = [1.0 / 32, 1.0 / 16, 1.0 / 8, 1.0 / 4, 1.0]\n        vgg = tf.keras.applications.VGG19(include_top=False, weights=\"imagenet\")\n        layer_outputs = [vgg.get_layer(x).output for x in self.encoder_layers]\n        self.vgg_model = tf.keras.Model(vgg.input, layer_outputs, name=\"VGG\")\n        self.mae = tf.keras.losses.MeanAbsoluteError()\n\n    def call(self, y_true, y_pred):\n        y_true = tf.keras.applications.vgg19.preprocess_input(255. * y_true)\n        y_pred = tf.keras.applications.vgg19.preprocess_input(255. * y_pred)\n        real_features = self.vgg_model(y_true)\n        fake_features = self.vgg_model(y_pred)\n        loss = 0\n        for i in range(len(real_features)):\n            loss += self.weights[i] * self.mae(real_features[i], fake_features[i])\n        return loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:45:39.402527Z","iopub.execute_input":"2025-02-03T09:45:39.402810Z","iopub.status.idle":"2025-02-03T09:45:39.408976Z","shell.execute_reply.started":"2025-02-03T09:45:39.402788Z","shell.execute_reply":"2025-02-03T09:45:39.408250Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SobelMaskingLoss(tf.keras.losses.Loss):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n    def call(self, y_true, y_pred):\n        mask_sobel = tf.image.sobel_edges(y_true)\n        mask_sobel = mask_sobel[..., 0] + mask_sobel[..., 1]\n        mask_sobel = tf.add(mask_sobel, \n                            tf.math.abs(tf.math.reduce_min(mask_sobel, axis=[1, 2, 3]))[..., tf.newaxis, tf.newaxis, tf.newaxis]) + 1.0\n        loss = tf.reduce_mean(tf.math.abs(y_true - y_pred) * mask_sobel)\n        return loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:45:39.560993Z","iopub.execute_input":"2025-02-03T09:45:39.561199Z","iopub.status.idle":"2025-02-03T09:45:39.565591Z","shell.execute_reply.started":"2025-02-03T09:45:39.561181Z","shell.execute_reply":"2025-02-03T09:45:39.564929Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SobelLoss(tf.keras.losses.Loss):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n    def call(self, y_true, y_pred):\n        sobel_true = tf.image.sobel_edges(y_true)\n        sobel_pred = tf.image.sobel_edges(y_pred)\n        loss = tf.reduce_mean(tf.math.abs(sobel_true - sobel_pred))\n        return loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:45:39.715137Z","iopub.execute_input":"2025-02-03T09:45:39.715340Z","iopub.status.idle":"2025-02-03T09:45:39.719514Z","shell.execute_reply.started":"2025-02-03T09:45:39.715323Z","shell.execute_reply":"2025-02-03T09:45:39.718841Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class PerceptualLoss(tf.keras.losses.Loss):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.vggloss = VGGFeatureMatchingLoss()\n        self.sobel_loss = SobelMaskingLoss()\n\n    def call(self, y_true, y_pred):\n        mask_sobel = tf.image.sobel_edges(y_true)\n        loss = (self.vggloss(y_true, y_pred) * 0.5\n                + self.sobel_loss(y_true, y_pred) * 10.\n                + (1 - tf.reduce_mean(tf.image.ssim(tf.clip_by_value(y_true, clip_value_max=1.0, clip_value_min=0.0), \n                                                    tf.clip_by_value(y_pred, clip_value_max=1.0, clip_value_min=0.0), \n                                                    1.0))) * 3.)\n        return loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:45:44.222646Z","iopub.execute_input":"2025-02-03T09:45:44.223090Z","iopub.status.idle":"2025-02-03T09:45:44.229753Z","shell.execute_reply.started":"2025-02-03T09:45:44.223048Z","shell.execute_reply":"2025-02-03T09:45:44.228886Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def charbonnier_loss(y_true, y_pred):\n    return tf.reduce_mean(tf.sqrt(tf.square(y_true - y_pred) + tf.square(1e-3)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:45:44.357064Z","iopub.execute_input":"2025-02-03T09:45:44.357304Z","iopub.status.idle":"2025-02-03T09:45:44.361153Z","shell.execute_reply.started":"2025-02-03T09:45:44.357284Z","shell.execute_reply":"2025-02-03T09:45:44.360327Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def lr_warmup_cosine_decay(global_step,\n                           warmup_steps,\n                           hold = 0,\n                           total_steps=0,\n                           start_lr=0.0,\n                           target_lr=1e-3):\n    # Cosine decay\n    learning_rate = 0.5 * target_lr * (1 + np.cos(np.pi * (global_step - warmup_steps - hold) / float(total_steps - warmup_steps - hold)))\n\n    # Target LR * progress of warmup (=1 at the final warmup step)\n    warmup_lr = target_lr * (global_step / warmup_steps)\n\n    # Choose between `warmup_lr`, `target_lr` and `learning_rate` based on whether `global_step < warmup_steps` and we're still holding.\n    # i.e. warm up if we're still warming up and use cosine decayed lr otherwise\n    if hold > 0:\n        learning_rate = np.where(global_step > warmup_steps + hold,\n                                 learning_rate, target_lr)\n    \n    learning_rate = np.where(global_step < warmup_steps, warmup_lr, learning_rate)\n    return learning_rate\n\nclass WarmupCosineDecay(tf.keras.callbacks.Callback):\n    def __init__(self, total_steps=0, warmup_steps=0, start_lr=0.0, target_lr=1e-3, hold=0):\n\n        super(WarmupCosineDecay, self).__init__()\n        self.start_lr = start_lr\n        self.hold = hold\n        self.total_steps = total_steps\n        self.global_step = 0\n        self.target_lr = target_lr\n        self.warmup_steps = warmup_steps\n        self.lrs = []\n\n    def on_batch_end(self, batch, logs=None):\n        self.global_step = self.global_step + 1\n        lr = model.optimizer.learning_rate.numpy()\n        self.lrs.append(lr)\n\n    def on_batch_begin(self, batch, logs=None):\n        lr = lr_warmup_cosine_decay(global_step=self.global_step,\n                                    total_steps=self.total_steps,\n                                    warmup_steps=self.warmup_steps,\n                                    start_lr=self.start_lr,\n                                    target_lr=self.target_lr,\n                                    hold=self.hold)\n        self.model.optimizer.learning_rate = lr","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:45:44.517234Z","iopub.execute_input":"2025-02-03T09:45:44.517481Z","iopub.status.idle":"2025-02-03T09:45:44.526498Z","shell.execute_reply.started":"2025-02-03T09:45:44.517460Z","shell.execute_reply":"2025-02-03T09:45:44.525544Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:45:54.788856Z","iopub.execute_input":"2025-02-03T09:45:54.789147Z","iopub.status.idle":"2025-02-03T09:45:54.795313Z","shell.execute_reply.started":"2025-02-03T09:45:54.789125Z","shell.execute_reply":"2025-02-03T09:45:54.794492Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# añadir guardar mejor modelo. Si esta 10 epochs sin bajar la loss reduzca learning rate. Reduce lr on plateau\n# 400 epochs\n# guardar cada vez que se reduzca la loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:45:55.039415Z","iopub.execute_input":"2025-02-03T09:45:55.039682Z","iopub.status.idle":"2025-02-03T09:45:55.043061Z","shell.execute_reply.started":"2025-02-03T09:45:55.039661Z","shell.execute_reply":"2025-02-03T09:45:55.042085Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"checkpoint = ModelCheckpoint(\n    \"best_model_unet_continue.weights.h5\", monitor=\"loss\", save_best_only=True, mode=\"min\", verbose=1, save_weights_only=True\n)\n\nreduce_lr = ReduceLROnPlateau(\n    monitor=\"loss\", factor=0.5, patience=10, min_lr=1e-6, verbose=1\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:45:55.283570Z","iopub.execute_input":"2025-02-03T09:45:55.284013Z","iopub.status.idle":"2025-02-03T09:45:55.290274Z","shell.execute_reply.started":"2025-02-03T09:45:55.283974Z","shell.execute_reply":"2025-02-03T09:45:55.289025Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n    loss=PerceptualLoss(),\n    metrics=[\"mae\", \"mse\"]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:45:55.526198Z","iopub.execute_input":"2025-02-03T09:45:55.526450Z","iopub.status.idle":"2025-02-03T09:45:56.437571Z","shell.execute_reply.started":"2025-02-03T09:45:55.526429Z","shell.execute_reply":"2025-02-03T09:45:56.436926Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.load_weights(PATH_MODEL) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:47:04.424711Z","iopub.execute_input":"2025-02-03T09:47:04.425073Z","iopub.status.idle":"2025-02-03T09:47:07.487423Z","shell.execute_reply.started":"2025-02-03T09:47:04.425044Z","shell.execute_reply":"2025-02-03T09:47:07.486607Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n    loss=VGGFeatureMatchingLoss(),\n    metrics=[\"mae\", \"mse\"]\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.fit(\n    train_generator,\n    epochs=100,\n    validation_data=test_generator,\n    callbacks=[checkpoint, reduce_lr],\n    verbose=1\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T14:22:33.514979Z","iopub.execute_input":"2025-02-02T14:22:33.515316Z","iopub.status.idle":"2025-02-02T14:57:52.111800Z","shell.execute_reply.started":"2025-02-02T14:22:33.515286Z","shell.execute_reply":"2025-02-02T14:57:52.110881Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), -> este\n              loss=PerceptualLoss(),\n              #loss=\"mae\",\n              metrics=[\"mae\", \"mse\"])\n'''\n\n'''\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n              loss={\"output1\": PerceptualLoss(), \n                    \"output2\": VGGFeatureMatchingLoss()},\n              metrics={\"output1\": [\"mae\", \"mse\"],\n                       \"output2\": [\"mae\", \"mse\"]})\n\ncallback = WarmupCosineDecay(total_steps=143 * 50, \n                             warmup_steps=int(143 * 50 * 0.05),\n                             hold=0, \n                             start_lr=0.0, \n                             target_lr=1e-3)\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model.save(\"./00_model_unetv1_v2.0.h5\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T14:57:57.587856Z","iopub.execute_input":"2025-02-02T14:57:57.588144Z","iopub.status.idle":"2025-02-02T14:57:58.545490Z","shell.execute_reply.started":"2025-02-02T14:57:57.588122Z","shell.execute_reply":"2025-02-02T14:57:58.544473Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Predict","metadata":{}},{"cell_type":"code","source":"X_rgb = np.asarray(Image.open(f\"{PATH_TEST}/5/rgb/5.png\"))[np.newaxis, ...]\nX_depth = np.asarray(Image.open(f\"{PATH_TEST}/5/depth/5.png\"))[np.newaxis, ...]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:53:20.980510Z","iopub.execute_input":"2025-02-03T09:53:20.980805Z","iopub.status.idle":"2025-02-03T09:53:21.027650Z","shell.execute_reply.started":"2025-02-03T09:53:20.980782Z","shell.execute_reply":"2025-02-03T09:53:21.027000Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_rgb = X_rgb / 255.\nX_depth = ((X_depth - np.min(X_depth)) / (np.max(X_depth) - np.min(X_depth)))[..., np.newaxis]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:53:21.206083Z","iopub.execute_input":"2025-02-03T09:53:21.206383Z","iopub.status.idle":"2025-02-03T09:53:21.212397Z","shell.execute_reply.started":"2025-02-03T09:53:21.206361Z","shell.execute_reply":"2025-02-03T09:53:21.211552Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = model.predict([X_rgb, X_depth])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:53:21.434325Z","iopub.execute_input":"2025-02-03T09:53:21.434613Z","iopub.status.idle":"2025-02-03T09:53:21.590555Z","shell.execute_reply.started":"2025-02-03T09:53:21.434590Z","shell.execute_reply":"2025-02-03T09:53:21.589638Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.imshow(X_rgb[0])\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:53:21.687468Z","iopub.execute_input":"2025-02-03T09:53:21.687757Z","iopub.status.idle":"2025-02-03T09:53:21.956722Z","shell.execute_reply.started":"2025-02-03T09:53:21.687736Z","shell.execute_reply":"2025-02-03T09:53:21.955757Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.imshow(y_pred[0])\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T09:53:28.542048Z","iopub.execute_input":"2025-02-03T09:53:28.542345Z","iopub.status.idle":"2025-02-03T09:53:28.785420Z","shell.execute_reply.started":"2025-02-03T09:53:28.542323Z","shell.execute_reply":"2025-02-03T09:53:28.784502Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = np.clip(y_pred, a_max=1., a_min=0.) * 255.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.imshow(y_pred[0].astype(np.uint16))\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Predict all","metadata":{}},{"cell_type":"code","source":"import os\nnewpath = r'./00_res' \nif not os.path.exists(newpath):\n    os.makedirs(newpath)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T10:03:32.845093Z","iopub.execute_input":"2025-02-03T10:03:32.845435Z","iopub.status.idle":"2025-02-03T10:03:32.849673Z","shell.execute_reply.started":"2025-02-03T10:03:32.845404Z","shell.execute_reply":"2025-02-03T10:03:32.848938Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(15):\n    print(i + 1)\n    X_rgb = np.asarray(Image.open(f\"{PATH_TEST}/{i+1}/rgb/{i+1}.png\"))[np.newaxis, ...]\n    X_depth = np.asarray(Image.open(f\"{PATH_TEST}/{i+1}/depth/{i+1}.png\"))[np.newaxis, ...]\n    \n    X_rgb = X_rgb / 255.\n    X_depth = ((X_depth - np.min(X_depth)) / (np.max(X_depth) - np.min(X_depth)))[..., np.newaxis]\n    \n    y_pred = model.predict([X_rgb, X_depth])\n\n    array = y_pred[0]\n\n    if array.dtype != np.uint8:\n        array = (array * 255).clip(0, 255).astype(np.uint8)\n\n    image = Image.fromarray(array)\n    image.save(f'./00_res/{i+1}.png')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T10:03:33.010119Z","iopub.execute_input":"2025-02-03T10:03:33.010389Z","iopub.status.idle":"2025-02-03T10:03:37.619135Z","shell.execute_reply.started":"2025-02-03T10:03:33.010368Z","shell.execute_reply":"2025-02-03T10:03:37.618432Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}